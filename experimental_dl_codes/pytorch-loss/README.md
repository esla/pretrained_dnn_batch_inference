# pytorch-loss

My implementation of label-smooth, amsoftmax, focal-loss, dual-focal-loss, triplet-loss, giou-loss, affinity-loss, pc_softmax_cross_entropy, and dice-loss(both generalized soft dice loss and batch soft dice loss). Maybe this is useful in my future work.


Also tried to implement swish and mish activation functions.

Additionally, one-hot function is added.

Newly add an "Exponential Moving Average(EMA)" operator.

Some operators are implemented with pytorch cuda extension, if you want to try them, please make sure you compile it first: 
```
    $ python setup.py install
```


For those who happen to find this repo, if you see errors in my code, feel free to open an issue to correct me.
